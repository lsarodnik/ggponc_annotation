{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproccess ASSESS CT data\n",
    "This processes the original ASSESS CT .xlxs file as published by (Miñarro-Giménez et al., 2019). The data filtered for German texts only and split up to be exported into .txt files to allow further usage e.g. for manual annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Language Annotator_ID Snippet_ID  Sentence_ID     Token Chunk SCT_ONLY CODE  \\\n",
      "0       EN          EN1        NL0            0  Exercise     1           NaN   \n",
      "1       EN          EN1        NL0            0  capacity     1           NaN   \n",
      "2       EN          EN1        NL0            0        is     1           NaN   \n",
      "3       EN          EN1        NL0            0     stage     1           NaN   \n",
      "4       EN          EN1        NL0            0         2     1           NaN   \n",
      "\n",
      "   SCT_ONLY CONCEPT COVERAGE SCT_ONLY TERM COVERAGE       UMLS_EXT CODE  \\\n",
      "0                        3.0                    NaN            C0948372   \n",
      "1                        3.0                    NaN            C0948372   \n",
      "2                        NaN                    NaN                 NaN   \n",
      "3                        3.0                    NaN  C1882085, C1882086   \n",
      "4                        3.0                    NaN            C1882085   \n",
      "\n",
      "   UMLS_EXT CONCEPT COVERAGE UMLS_EXT TERM COVERAGE LOCAL CODE  \\\n",
      "0                        1.0                    yes        NaN   \n",
      "1                        1.0                    yes        NaN   \n",
      "2                        NaN                    NaN        NaN   \n",
      "3                        0.0                    yes        NaN   \n",
      "4                        0.0                    yes        NaN   \n",
      "\n",
      "   LOCAL CONCEPT COVERAGE LOCAL TERM COVERAGE  \n",
      "0                     NaN                 NaN  \n",
      "1                     NaN                 NaN  \n",
      "2                     NaN                 NaN  \n",
      "3                     NaN                 NaN  \n",
      "4                     NaN                 NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('../data/ASSESS CT/pone.0209547.s002.xlsx', header=[0,1])\n",
    "df.columns = [f'{a} {b}' if not b.startswith('Unnamed:') else a for a, b in df.columns]\n",
    "\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 1817 Tokens Annotated by DE1 are also part of DE2: True\n",
      "All 5486 Tokens Annotated by DE2 are also part of DE1:  False\n"
     ]
    }
   ],
   "source": [
    "#Filtering\n",
    "filtered_DE_df = df[df['Language'] == 'DE']\n",
    "\n",
    "filtered_DE1_df = df[df['Annotator_ID'] == 'DE1']\n",
    "filtered_DE2_df = df[df['Annotator_ID'] == 'DE2']\n",
    "\n",
    "#check possible duplicates / overlaps in annotated datasets\n",
    "print(\"All %d Tokens Annotated by DE1 are also part of DE2: %s\" % (len(filtered_DE1_df), filtered_DE1_df['Token'].isin(filtered_DE2_df['Token']).all()))\n",
    "print(\"All %0d Tokens Annotated by DE2 are also part of DE1:  %s\" % (len(filtered_DE2_df), filtered_DE2_df['Token'].isin(filtered_DE1_df['Token']).all()))\n",
    "\n",
    "# --> Proceeding with filtered_DE2_df only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60.00000</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.55000</td>\n",
       "      <td>91.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.10632</td>\n",
       "      <td>16.451564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.00000</td>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.00000</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.00000</td>\n",
       "      <td>91.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.00000</td>\n",
       "      <td>99.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>25.00000</td>\n",
       "      <td>138.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentences      Tokens\n",
       "count   60.00000   60.000000\n",
       "mean     9.55000   91.416667\n",
       "std      4.10632   16.451564\n",
       "min      3.00000   48.000000\n",
       "25%      7.00000   84.000000\n",
       "50%      9.00000   91.000000\n",
       "75%     12.00000   99.250000\n",
       "max     25.00000  138.000000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Insights on sentence and token structure\n",
    "sentences_token_per_document = filtered_DE2_df.groupby(['Snippet_ID']).agg({'Sentence_ID': 'max', 'Token': 'count'})\n",
    "sentences_token_per_document = sentences_token_per_document.rename(columns={'Sentence_ID': 'Sentences', 'Token': 'Tokens'})\n",
    "sentences_token_per_document['Sentences'] += 1 #Take care of Null Indices\n",
    "sentences_token_per_document.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export into .txt files\n",
    "import os\n",
    "\n",
    "output_dir = './txt_output'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for snippet_id in filtered_DE2_df['Snippet_ID'].unique():\n",
    "    snippet_df = filtered_DE2_df[filtered_DE2_df['Snippet_ID'] == snippet_id]\n",
    "    first_line = True\n",
    "    with open(f\"{output_dir}/{snippet_id}.txt\", \"w\") as f:\n",
    "        for sentence_id in snippet_df['Sentence_ID'].unique():\n",
    "            sentence_df = snippet_df[snippet_df['Sentence_ID'] == sentence_id]\n",
    "            sentence_tokens = sentence_df['Token'].astype(str).tolist()\n",
    "            sentence = ' '.join(sentence_tokens)\n",
    "            if first_line:\n",
    "                f.write(sentence)\n",
    "                first_line = False\n",
    "            else:\n",
    "                f.write('\\n' + sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
