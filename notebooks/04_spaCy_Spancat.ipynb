{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dd282e9-2b3b-4319-9eb0-d60b9fd968f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prediction of overlapping spans with spaCy's SpanCategorizer\n",
    "\n",
    "**Motivation**:\n",
    "\n",
    "Annotations in GGPONC are often overlapping or nested.\n",
    "\n",
    "For instance, `Versagen einer Behandlung mit Oxaliplatin und Irinotecan`\n",
    "- is a *Finding*\n",
    "- which contains a *Therapeutic Procedure*: `Behandlung mit Oxaliplatin und Irinotecan`:\n",
    "    - which in turn contains two *Clinical Drug* names: (`Oxaliplatin` and `Irinotecan`).\n",
    "\n",
    "Standard IOB-encoded labels, and most NER implementations, can only model one label per token, so by default we consider the longest surrounding mention span only in the IOB-based / HuggingFace implementation (in this case, the *Finding*).\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "Instead of token-level labels, we use spaCy's new [SpanCategorizer](https://spacy.io/api/spancategorizer/) implementation to predict overlapping mention spans as a SpanGroup in a spaCy document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add8d33-0eaa-47f7-856d-630e54581a7b",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "See the `spacy` folder in the root directory of the project. The model configuration can be found at `configs` and training can be run through a spaCy project (see `spacy/run_training.sh`). \n",
    "\n",
    "*Note:* We have currently not optimized the many hyperparameters related to span suggestion and model training. However, performance is close to the HuggingFace models evaluated on non-nested mention spans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0cd5b1-300e-4710-a85f-e2413bb33df7",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "357713b1-2285-4923-826b-f7e58178c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f14a1448-ab8c-4cf7-9fd8-c728b21514f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, Span\n",
    "import snomed_spans #TODO: import needed to enable custom spaCy components, is there another way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c23f22a8-c73d-4508-a756-f220d167e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('../data/models/spacy')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e211c2b5",
   "metadata": {},
   "source": [
    "### Grascco Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20601a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"\"\"6.04.2029: Nachdebridement am Kopf, VAG-Wechsel linke Hand\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f493809",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"6.04.2029: Nachdebridement am Kopf, VAG-Wechsel linke Hand\"\"Röntgen : Rippstein I : Gute Hüftkopfepiphysenkonturgebung , minimale Lateralisation , li. etwas stärker als re. , noch übergreifende Pfannendächer , Shenton-Menard-Linie nicht wesentlich unterbrochen , Pfannendachwinkel Ii. 30° , re. ebenfalls knapp 30° .\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15624a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sorted(list(doc.spans['snomed']), key=lambda s: s.start):\n",
    "    print(s, s.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc426969",
   "metadata": {},
   "source": [
    "## Initial Sentence Based Processing + Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a60a0f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "nlp = spacy.load('../data/models/spacy')\n",
    "\n",
    "def find_annotated_entities(sentence, document):\n",
    "    s_offsets = sentence['offsets']\n",
    "    entity_list = []\n",
    "    for entity in document['entities']:\n",
    "        if entity is None:\n",
    "            break\n",
    "        elif(entity['offsets'][0][0] > s_offsets[0][1]): \n",
    "            break\n",
    "        elif (entity['offsets'][0][0] >=  s_offsets[0][0] and entity['offsets'][0][1] <= s_offsets[0][1]):\n",
    "            entity_list.append(entity)\n",
    "    return entity_list\n",
    "\n",
    "tp = 0\n",
    "e_count = 0\n",
    "t_count = 0\n",
    "\n",
    "def compare_findings(predi_entities, truth_entities, sentence):\n",
    "    sentence_delta = sentence['offsets'][0][0]\n",
    "    global tp, e_count, t_count\n",
    "    for e in list(predi_entities.spans['snomed']):\n",
    "        e_count+=1\n",
    "        for t in truth_entities:\n",
    "            if e.label_ == t['type'] and e.start_char == t['offsets'][0][0]-sentence_delta and e.end_char == t['offsets'][0][1]-sentence_delta:\n",
    "                tp+=1\n",
    "                break\n",
    "    t_count += len(truth_entities)\n",
    "\n",
    "\n",
    "json_path = \"/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/GraSSco/grassco_anno_2023-01-05_0021/json/fine/long/test.json\"\n",
    "json_path_new = \"/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/GraSSco/grascco_hpi_anno_2023_02_08/annotations/json/fine/long/all_short.json\"\n",
    "\n",
    "with open(json_path_new) as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "\n",
    "for document in data:\n",
    "    print(\"Current document_id: \"+document['document_id'])\n",
    "    for sentence in document['passages']:\n",
    "        nlp_findings = nlp(sentence['text'], disable=[\"tok2vec\", \"tagger\", \"attribute_ruler\",\"lemmatizer\"]) #disable_components (standard NER tagger etc.) - \"parser\" is helpful (10 matches less when not used)\n",
    "        manual_findings = find_annotated_entities(sentence, document)\n",
    "        compare_findings(nlp_findings, manual_findings, sentence)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8948f5eb",
   "metadata": {},
   "source": [
    "## Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c45935ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Extract paths to configuration files\n",
    "CONFIG = {\n",
    "    'json_path':'../data/GraSSco/grassco_anno_2023-01-05_0021/json/fine/long/test.json',\n",
    "    'json_path_new': '../data/GraSSco/grascco_hpi_anno_2023_02_08/annotations/json/fine/long/all_short.json',\n",
    "    'ggponc_full_set': '../data/annotations/json/fine/long/all.json',\n",
    "    'train_test_split': '../data/annotations/splits.csv'\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ebc77ae",
   "metadata": {},
   "source": [
    "### Get GGPONC Test Data set as given in splits.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b8178530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "# Open the CSV file and read its contents into a dictionary\n",
    "with open(CONFIG['train_test_split'], 'r') as csvfile:\n",
    "    next(csvfile)\n",
    "    csvdata = {row[0]: row[1] for row in csv.reader(csvfile)}\n",
    "\n",
    "test_ids = set()\n",
    "# Get the set of test file IDs\n",
    "for file_name, split_value in csvdata.items():\n",
    "    if split_value == 'test':\n",
    "        test_ids.add(os.path.splitext(file_name)[0])\n",
    "\n",
    "\n",
    "# Load the JSON data from the file and filter it for the test split\n",
    "with open(CONFIG['ggponc_full_set'], 'r') as jsonfile:\n",
    "    jsondata = json.load(jsonfile)\n",
    "    test_data = [doc for doc in jsondata if os.path.splitext(doc['document_id'])[0] in test_ids]\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33d93d5c",
   "metadata": {},
   "source": [
    "## Sentence Based NER processing and saving result as BigBio (json) (predicted entities only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd8b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CONFIG['json_path']) as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "predicted_findings = []\n",
    "\n",
    "for document in data:\n",
    "    print(\"Current document_id: \" + document['document_id'])\n",
    "    entities = []\n",
    "    # use pipe, disable_components in pipe (standard NER tagger etc.) - \"parser\" is helpful (10 matches less when not used)\n",
    "    for i, doc in enumerate(nlp.pipe([passage['text'] for passage in document['passages'] if 'text' in passage], disable=[\"tok2vec\", \"tagger\", \"attribute_ruler\",\"lemmatizer\"])):\n",
    "        entity = []\n",
    "        sentence_offset = document['passages'][i]['offsets'][0][0]\n",
    "        for ent in doc.spans['snomed']:\n",
    "            entity.append({\n",
    "            \"text\": [ent.text],\n",
    "            \"type\": ent.label_,\n",
    "            \"offsets\": [[ent.start_char+sentence_offset,ent.end_char+sentence_offset]]\n",
    "        })\n",
    "\n",
    "        entities.extend(entity)\n",
    "        \n",
    "\n",
    "    output_dict = {\n",
    "        \"document_id\": document['document_id'],\n",
    "        \"entities\": entities\n",
    "    }\n",
    "    \n",
    "    predicted_findings.append(output_dict)\n",
    "    \n",
    "# Write the output to json file\n",
    "import os\n",
    "output_path = os.path.splitext(json_file.name)[0] + '_NER_processed.json'\n",
    "\n",
    "with open(output_path, \"w\") as outfile:\n",
    "    json.dump(predicted_findings, outfile,ensure_ascii=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f164d53",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c73855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load predicted json data\n",
    "with open('../data/GraSSco/grassco_anno_2023-01-05_0021/json/fine/long/test_NER_processed.json') as json_file:\n",
    "    predicted_findings = json.load(json_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e62eccb",
   "metadata": {},
   "source": [
    "### Traditional Error Analysis (F1 Score based on TP, FN, FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b22068be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import util\n",
    "\n",
    "def compare_findings(pred_entities, truth_entities):\n",
    "    true_positive_count = sum(1 for e in pred_entities if any(e['type'] == t['type'] and\n",
    "                                     e['offsets'] == t['offsets']\n",
    "                                     for t in truth_entities))\n",
    "    return true_positive_count, len(pred_entities), len(truth_entities)\n",
    "\n",
    "\n",
    "with open(CONFIG['json_path']) as json_file:\n",
    "    data_t = json.load(json_file)\n",
    "\n",
    "tp = e_count = t_count = 0\n",
    "\n",
    "for i_doc, doc_p in enumerate(predicted_findings):\n",
    "        tp_i, e_count_i, t_count_i, = compare_findings(doc_p['entities'], data[i_doc]['entities'])\n",
    "        tp += tp_i\n",
    "        e_count += e_count_i\n",
    "        t_count += t_count_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b19691b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Entities: 6194\n",
      "Predicted Entities: 5171\n",
      "True Positives: 2584\n",
      "False Positives: 2587\n",
      "False Negatives: 3610\n",
      "Precision: 0.50\n",
      "Recall: 0.42\n",
      "F1 Score: 0.45\n"
     ]
    }
   ],
   "source": [
    "fn = t_count-tp\n",
    "fp = e_count-tp\n",
    "\n",
    "print(f'Actual Entities: {t_count}')\n",
    "print(f'Predicted Entities: {e_count}')\n",
    "\n",
    "print(f'True Positives: {tp}')\n",
    "print(f'False Positives: {fp}')\n",
    "print(f'False Negatives: {fn}')\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1_score:.2f}')\n",
    "\n",
    "\n",
    "#result for fine/long\n",
    "\n",
    "#Actual Entities: 6194\n",
    "#Predicted Entities: 5171\n",
    "#True Positives: 2584\n",
    "#False Positives: 2587\n",
    "#False Negatives: 3610\n",
    "#Precision: 0.50\n",
    "#Recall: 0.42\n",
    "#F1 Score: 0.45"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3652b8a6",
   "metadata": {},
   "source": [
    "### NER context related Error Analysis (based on FairEval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d4762e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../experiments')\n",
    "\n",
    "from error_analysis import _ner_error_analyis\n",
    "\n",
    "def compare_findings(pred_entities, truth_entities):\n",
    "    pred_spans = []\n",
    "    gt_spans = []\n",
    "    \n",
    "    for p in pred_entities:\n",
    "        pred_spans.append({\n",
    "            'start': p['offsets'][0][0],\n",
    "            'end': p['offsets'][0][1],\n",
    "            'entity_group': p['type'],\n",
    "            'word': p['text'][0]\n",
    "        })\n",
    "\n",
    "    for t in truth_entities:\n",
    "        gt_spans.append({\n",
    "            'start': t['offsets'][0][0],\n",
    "            'end': t['offsets'][0][1],\n",
    "            'entity_group': t['type'],\n",
    "            'word': t['text'][0]\n",
    "        })\n",
    "    \n",
    "    return _ner_error_analyis(pred_spans, gt_spans)\n",
    "\n",
    "\n",
    "with open(CONFIG['json_path']) as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "errors = []\n",
    "\n",
    "for i_doc, doc_p in enumerate(predicted_findings):\n",
    "        errors_i = compare_findings(doc_p['entities'], data[i_doc]['entities'])\n",
    "        errors.extend(errors_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f5e2c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "boundary_error_larger          31\n",
       "boundary_error_overlap         14\n",
       "boundary_error_smaller        362\n",
       "boundary_error_span_count     171\n",
       "false_negative               2853\n",
       "false_positive                952\n",
       "label_boundary_error          774\n",
       "labeling_error                337\n",
       "true_positive                1590\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get stats and export errors as csv\n",
    "import pandas as pd\n",
    "stats_df = pd.DataFrame(errors)\n",
    "\n",
    "error_count = stats_df.groupby('category').size()\n",
    "stats_df.to_csv('error_analysis.csv')\n",
    "error_count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9478a3e2",
   "metadata": {},
   "source": [
    "## Document Based Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d39e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#nlp = spacy.load('../data/models/spacy')\n",
    "\n",
    "def get_string_sentences(docs):\n",
    "    return ['\\n'.join([sentence['text'] for sentence in doc['passages']]) for doc in docs]\n",
    "\n",
    "def find_annotated_entities(annotated_doc):\n",
    "    return [entity for entity in annotated_doc['entities'] if entity]\n",
    "\n",
    "def compare_findings(predicted_findings, truth_entities):\n",
    "    predicted_findings = sorted(predicted_findings, key=lambda x: x.start_char)\n",
    "    truth_set = set(t['offsets'][0][0] for t in truth_entities)\n",
    "    tp, e_count, t_count = 0, len(predicted_findings), len(truth_entities)\n",
    "    for e in predicted_findings:\n",
    "        if e.start_char in truth_set:\n",
    "            t = next(t for t in truth_entities if t['offsets'][0][0] == e.start_char)\n",
    "            if e.label_ == t['type'] and e.start_char == t['offsets'][0][0] and e.end_char == t['offsets'][0][1]:\n",
    "                tp += 1\n",
    "    return tp, e_count, t_count\n",
    "\n",
    "tp, e_count, t_count = 0, 0, 0\n",
    "\n",
    "json_path = \"../data/GraSSco/grassco_anno_2023-01-05_0021/json/fine/long/test.json\"\n",
    "json_path_new = \"../data/GraSSco/grascco_hpi_anno_2023_02_08/annotations/json/fine/long/all_short.json\"\n",
    "ggponc_test_set = \"../data/annotations/json/fine/long/all.json\"\n",
    "\n",
    "with open(ggponc_test_set) as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "print(\"Processing all documents... this takes up to multiple minutes hang tight!\")\n",
    "\n",
    "# use pipe, disable_components in pipe (standard NER tagger etc.) - \"parser\" is helpful (10 matches less when not used)\n",
    "for i, doc in enumerate(nlp.pipe(get_string_sentences(data), disable=[\"tok2vec\", \"attribute_ruler\", \"lemmatizer\"])):\n",
    "    predicted_findings = [(ent) for ent in list(doc.spans['snomed'])]\n",
    "    manual_findings = find_annotated_entities(data[i])\n",
    "    tp_i, e_count_i, t_count_i = compare_findings(predicted_findings, manual_findings)\n",
    "    tp += tp_i\n",
    "    e_count += e_count_i\n",
    "    t_count += t_count_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a36fd872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Entities: 195\n",
      "Predicted Entities: 177\n",
      "True Positives: 81\n",
      "False Positives: 96\n",
      "False Negatives: 114\n",
      "Precision: 0.46\n",
      "Recall: 0.42\n",
      "F1 Score: 0.44\n"
     ]
    }
   ],
   "source": [
    "fn = t_count-tp\n",
    "fp = e_count-tp\n",
    "\n",
    "print(f'Actual Entities: {t_count}')\n",
    "print(f'Predicted Entities: {e_count}')\n",
    "\n",
    "print(f'True Positives: {tp}')\n",
    "print(f'False Positives: {fp}')\n",
    "print(f'False Negatives: {fn}')\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1_score:.2f}')\n",
    "\n",
    "\n",
    "#sentence based result for all-short\n",
    "\n",
    "#Actual Entities: 195\n",
    "#Predicted Entities: 187\n",
    "#True Positives: 87\n",
    "#False Positives: 100\n",
    "#False Negatives: 108\n",
    "#Precision: 0.47\n",
    "#Recall: 0.45\n",
    "#F1 Score: 0.46\n",
    "\n",
    "#document based result for all-short\n",
    "\n",
    "#Actual Entities: 195\n",
    "#Predicted Entities: 177\n",
    "#True Positives: 81\n",
    "#False Positives: 96\n",
    "#False Negatives: 114\n",
    "#Precision: 0.46\n",
    "#Recall: 0.42\n",
    "#F1 Score: 0.44"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d3dcc27",
   "metadata": {},
   "source": [
    "## Spacy Native Eval Approach (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc_bin = DocBin().from_disk(manual_annotated_file)\n",
    "\n",
    "\n",
    "gold_docs  = list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "#scorer = Scorer()\n",
    "#scores = scorer.score(examples)\n",
    "\n",
    "#gold_annotation = spacy.Corpus.v1(manual_annotated_file, gold_preproc=True)\n",
    "docs = []\n",
    "for file in files:\n",
    "    docs.append(file.read_text(encoding=\"utf-8\"))\n",
    "print(\"Files merged...\")\n",
    "\n",
    "\n",
    "# Loop over the gold standard data\n",
    "#for gold_doc in gold_docs:\n",
    "#    ents1 = [(gold_doc.text, gold_doc.label_) for ent in gold_doc.ents]\n",
    "    # Process the text with the model\n",
    "# Compare the model's predicted annotations with the gold standard\n",
    "#for doc in docs:\n",
    "#    ents2 = [(doc.text, doc.label_) for ent in doc.ents]\n",
    "\n",
    "print(\"NLP Pipe...\")\n",
    "docs_all = nlp.pipe(gold_docs, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"],batch_size = 10)\n",
    "\n",
    "\n",
    "print(gold_docs[1])\n",
    "print(\"Building Examples...\")\n",
    "examples = []\n",
    "for  i, doc in enumerate(docs_all):\n",
    "    examples.append(Example((doc), gold_docs[i]))\n",
    "    \n",
    "\n",
    "\n",
    "scorer = Scorer(nlp)\n",
    "\n",
    "print(\"eval...\")\n",
    "scores = scorer.score(examples)\n",
    "\n",
    "\n",
    "#example_object = iter(docs)\n",
    "\n",
    "\n",
    "#examples = Example(Doc.from_docs(docs), Doc.from_docs(gold_docs))\n",
    "\n",
    "scorer.score()\n",
    "\n",
    "\n",
    "#print(\"Entities F-Score:\", scorer.scores[\"ents_f\"])\n",
    "#print(\"Entities Precision:\", scorer.scores[\"ents_p\"])\n",
    "#print(\"Entities Recall:\", scorer.scores[\"ents_r\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b2feaa6",
   "metadata": {},
   "source": [
    "### Visualizing NER\n",
    "Spacy has a library called “displaCy” which helps us to explore the behaviour of the entity recognition model interactively.\n",
    "\n",
    "If you are training a model, it’s very useful to run the visualization yourself.\n",
    "\n",
    "You can pass a Doc or a list of Doc objects to displaCy and run displacy.serve to run the webserver, or displacy.render to generate the raw mark-up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2602bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Import the displaCy library\n",
    "from spacy import displacy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
