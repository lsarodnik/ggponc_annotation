{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dd282e9-2b3b-4319-9eb0-d60b9fd968f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prediction of overlapping spans with spaCy's SpanCategorizer\n",
    "\n",
    "**Motivation**:\n",
    "\n",
    "Annotations in GGPONC are often overlapping or nested.\n",
    "\n",
    "For instance, `Versagen einer Behandlung mit Oxaliplatin und Irinotecan`\n",
    "- is a *Finding*\n",
    "- which contains a *Therapeutic Procedure*: `Behandlung mit Oxaliplatin und Irinotecan`:\n",
    "    - which in turn contains two *Clinical Drug* names: (`Oxaliplatin` and `Irinotecan`).\n",
    "\n",
    "Standard IOB-encoded labels, and most NER implementations, can only model one label per token, so by default we consider the longest surrounding mention span only in the IOB-based / HuggingFace implementation (in this case, the *Finding*).\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "Instead of token-level labels, we use spaCy's new [SpanCategorizer](https://spacy.io/api/spancategorizer/) implementation to predict overlapping mention spans as a SpanGroup in a spaCy document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add8d33-0eaa-47f7-856d-630e54581a7b",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "See the `spacy` folder in the root directory of the project. The model configuration can be found at `configs` and training can be run through a spaCy project (see `spacy/run_training.sh`). \n",
    "\n",
    "*Note:* We have currently not optimized the many hyperparameters related to span suggestion and model training. However, performance is close to the HuggingFace models evaluated on non-nested mention spans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0cd5b1-300e-4710-a85f-e2413bb33df7",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "357713b1-2285-4923-826b-f7e58178c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f14a1448-ab8c-4cf7-9fd8-c728b21514f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, Span\n",
    "import snomed_spans #TODO: import needed to enable custom spaCy components, is there another way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c23f22a8-c73d-4508-a756-f220d167e7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/spacy/util.py:877: UserWarning: [W095] Model 'de_pipeline' (0.0.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.4.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('../data/models/spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "844d6044-d4f1-4d1d-b608-a0069d03617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"\"\"Versagen einer Behandlung mit Oxaliplatin und Irinotecan\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e211c2b5",
   "metadata": {},
   "source": [
    "### Grascco Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20601a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"\"\"6.04.2029: Nachdebridement am Kopf, VAG-Wechsel linke Hand\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f493809",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"6.04.2029: Nachdebridement am Kopf, VAG-Wechsel linke Hand\"\"Röntgen : Rippstein I : Gute Hüftkopfepiphysenkonturgebung , minimale Lateralisation , li. etwas stärker als re. , noch übergreifende Pfannendächer , Shenton-Menard-Linie nicht wesentlich unterbrochen , Pfannendachwinkel Ii. 30° , re. ebenfalls knapp 30° .\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f547e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading .spacy file ...\n",
      "working ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/notebooks/04_spaCy_Spancat.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/notebooks/04_spaCy_Spancat.ipynb#Y100sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m files:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/notebooks/04_spaCy_Spancat.ipynb#Y100sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     text \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread_text(encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/notebooks/04_spaCy_Spancat.ipynb#Y100sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m docs \u001b[39m=\u001b[39m nlp(text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from sklearn.metrics import f1_score\n",
    "from spacy.training import docs_to_json\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training import Corpus\n",
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "folder_raw = \"/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/GraSSco/source\"\n",
    "manual_annotated_file = \"/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/GraSSco/grassco_anno_2023-01-05_0021/spacy/test.spacy\"\n",
    "p = Path(r'/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/GraSSco/source').glob('*.txt')\n",
    "files = [x for x in p if x.is_file()]\n",
    "\n",
    "\n",
    "\n",
    "print(\"loading .spacy file ...\")\n",
    "#gold_annotation = nlp.from_disk(manual_annotated_file)\n",
    "#doc_bin = DocBin().from_disk(manual_annotated_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"working ...\")\n",
    "text = \"\"\n",
    "#text = \"\"\n",
    "for file in files:\n",
    "    text += file.read_text(encoding=\"utf-8\")\n",
    "docs = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5dcaf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files merged...\n",
      "NLP Pipe...\n",
      "Department Orthopädie und Traumatologie Friedrichstraße 55 , 10117 Berlin \n",
      "Building Examples...\n",
      "eval...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "score() missing 1 required positional argument: 'examples'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/notebooks/04_spaCy_Spancat.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/notebooks/04_spaCy_Spancat.ipynb#Y101sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m scores \u001b[39m=\u001b[39m scorer\u001b[39m.\u001b[39mscore(examples)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/notebooks/04_spaCy_Spancat.ipynb#Y101sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m#example_object = iter(docs)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/notebooks/04_spaCy_Spancat.ipynb#Y101sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/notebooks/04_spaCy_Spancat.ipynb#Y101sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/notebooks/04_spaCy_Spancat.ipynb#Y101sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m#examples = Example(Doc.from_docs(docs), Doc.from_docs(gold_docs))\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/notebooks/04_spaCy_Spancat.ipynb#Y101sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m scorer\u001b[39m.\u001b[39;49mscore()\n",
      "\u001b[0;31mTypeError\u001b[0m: score() missing 1 required positional argument: 'examples'"
     ]
    }
   ],
   "source": [
    "\n",
    "doc_bin = DocBin().from_disk(manual_annotated_file)\n",
    "\n",
    "\n",
    "gold_docs  = list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "#scorer = Scorer()\n",
    "#scores = scorer.score(examples)\n",
    "\n",
    "#gold_annotation = spacy.Corpus.v1(manual_annotated_file, gold_preproc=True)\n",
    "docs = []\n",
    "for file in files:\n",
    "    docs.append(file.read_text(encoding=\"utf-8\"))\n",
    "print(\"Files merged...\")\n",
    "\n",
    "\n",
    "# Loop over the gold standard data\n",
    "#for gold_doc in gold_docs:\n",
    "#    ents1 = [(gold_doc.text, gold_doc.label_) for ent in gold_doc.ents]\n",
    "    # Process the text with the model\n",
    "# Compare the model's predicted annotations with the gold standard\n",
    "#for doc in docs:\n",
    "#    ents2 = [(doc.text, doc.label_) for ent in doc.ents]\n",
    "\n",
    "print(\"NLP Pipe...\")\n",
    "docs_all = nlp.pipe(gold_docs, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"],batch_size = 10)\n",
    "\n",
    "\n",
    "print(gold_docs[1])\n",
    "print(\"Building Examples...\")\n",
    "examples = []\n",
    "for  i, doc in enumerate(docs_all):\n",
    "    examples.append(Example((doc), gold_docs[i]))\n",
    "    \n",
    "\n",
    "\n",
    "scorer = Scorer(nlp)\n",
    "\n",
    "print(\"eval...\")\n",
    "scores = scorer.score(examples)\n",
    "\n",
    "\n",
    "#example_object = iter(docs)\n",
    "\n",
    "\n",
    "#examples = Example(Doc.from_docs(docs), Doc.from_docs(gold_docs))\n",
    "\n",
    "scorer.score()\n",
    "\n",
    "\n",
    "#print(\"Entities F-Score:\", scorer.scores[\"ents_f\"])\n",
    "#print(\"Entities Precision:\", scorer.scores[\"ents_p\"])\n",
    "#print(\"Entities Recall:\", scorer.scores[\"ents_r\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03f72f97",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Scorer' object has no attribute 'scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/notebooks/04_spaCy_Spancat.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/notebooks/04_spaCy_Spancat.ipynb#Y102sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m scorer \u001b[39m=\u001b[39m Scorer()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/notebooks/04_spaCy_Spancat.ipynb#Y102sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m scores \u001b[39m=\u001b[39m scorer\u001b[39m.\u001b[39mscore(examples)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/notebooks/04_spaCy_Spancat.ipynb#Y102sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEntities F-Score:\u001b[39m\u001b[39m\"\u001b[39m, scorer\u001b[39m.\u001b[39;49mscores[\u001b[39m\"\u001b[39m\u001b[39ments_f\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/notebooks/04_spaCy_Spancat.ipynb#Y102sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEntities Precision:\u001b[39m\u001b[39m\"\u001b[39m, scorer\u001b[39m.\u001b[39mscores[\u001b[39m\"\u001b[39m\u001b[39ments_p\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/notebooks/04_spaCy_Spancat.ipynb#Y102sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEntities Recall:\u001b[39m\u001b[39m\"\u001b[39m, scorer\u001b[39m.\u001b[39mscores[\u001b[39m\"\u001b[39m\u001b[39ments_r\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Scorer' object has no attribute 'scores'"
     ]
    }
   ],
   "source": [
    "scorer = Scorer()\n",
    "\n",
    "scores = scorer.score(examples)\n",
    "\n",
    "print(\"Entities F-Score:\", scorer.scores[\"ents_f\"])\n",
    "print(\"Entities Precision:\", scorer.scores[\"ents_p\"])\n",
    "print(\"Entities Recall:\", scorer.scores[\"ents_r\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a007a0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/notebooks/04_spaCy_Spancat.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/notebooks/04_spaCy_Spancat.ipynb#Y103sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sentence[\u001b[39m'\u001b[39m\u001b[39moffsets\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentence' is not defined"
     ]
    }
   ],
   "source": [
    "sentence['offsets']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc426969",
   "metadata": {},
   "source": [
    "## Initial Sentence Based Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a60a0f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/spacy/util.py:877: UserWarning: [W095] Model 'de_pipeline' (0.0.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.4.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current document_id: Albers.tsv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "nlp = spacy.load('../data/models/spacy')\n",
    "\n",
    "def find_annotated_entities(sentence, document):\n",
    "    s_offsets = sentence['offsets']\n",
    "    entity_list = []\n",
    "    for entity in document['entities']:\n",
    "        if entity is None:\n",
    "            break\n",
    "        elif(entity['offsets'][0][0] > s_offsets[0][1]): \n",
    "            break\n",
    "        elif (entity['offsets'][0][0] >=  s_offsets[0][0] and entity['offsets'][0][1] <= s_offsets[0][1]):\n",
    "            entity_list.append(entity)\n",
    "    return entity_list\n",
    "\n",
    "tp = 0\n",
    "e_count = 0\n",
    "t_count = 0\n",
    "\n",
    "def compare_findings(predi_entities, truth_entities, sentence):\n",
    "    sentence_delta = sentence['offsets'][0][0]\n",
    "    global tp, e_count, t_count\n",
    "    for e in list(predi_entities.spans['snomed']):\n",
    "        e_count+=1\n",
    "        for t in truth_entities:\n",
    "            if e.label_ == t['type'] and e.start_char == t['offsets'][0][0]-sentence_delta and e.end_char == t['offsets'][0][1]-sentence_delta:\n",
    "                tp+=1\n",
    "                break\n",
    "    t_count += len(truth_entities)\n",
    "\n",
    "\n",
    "json_path = \"/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/GraSSco/grassco_anno_2023-01-05_0021/json/fine/long/test.json\"\n",
    "json_path_new = \"/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/GraSSco/grascco_hpi_anno_2023_02_08/annotations/json/fine/long/all_short.json\"\n",
    "\n",
    "with open(json_path_new) as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "\n",
    "for document in data:\n",
    "    print(\"Current document_id: \"+document['document_id'])\n",
    "    for sentence in document['passages']:\n",
    "        nlp_findings = nlp(sentence['text'], disable=[\"tok2vec\", \"tagger\", \"attribute_ruler\",\"lemmatizer\"]) #disable_components (standard NER tagger etc.) - \"parser\" is helpful (10 matches less when not used)\n",
    "        manual_findings = find_annotated_entities(sentence, document)\n",
    "        compare_findings(nlp_findings, manual_findings, sentence)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df884b6d",
   "metadata": {},
   "source": [
    "## Sentence Based Processing (Optimized) + 1st Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88770942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current document_id: Albers.tsv\n",
      "[(108, 124, 'Therapeutic', 'stat. Behandlung')]\n",
      "[(108, 124, 'Therapeutic', 'stat. Behandlung')]\n",
      "TP\n",
      "[]\n",
      "[(0, 13, 'Other_Finding', 'Vorgeschichte')]\n",
      "FN\n",
      "[(14, 20, 'Other_Finding', 'Befund')]\n",
      "[(14, 20, 'Other_Finding', 'Befund')]\n",
      "TP\n",
      "[(2, 28, 'Diagnosis_or_Pathology', 'Verbrennung 1. – 3. Grades')]\n",
      "[(2, 50, 'Diagnosis_or_Pathology', 'Verbrennung 1. – 3. Grades, Kopf I Hals,5% v KOF')]\n",
      "BE\n",
      "[(2, 16, 'Therapeutic', 'Handamputation')]\n",
      "[(2, 19, 'Diagnosis_or_Pathology', 'Handamputation LI')]\n",
      "LBE\n",
      "[(2, 50, 'Diagnosis_or_Pathology', 'Akute Psychose aus dem schizophrenen Formenkreis')]\n",
      "[(2, 50, 'Diagnosis_or_Pathology', 'Akute Psychose aus dem schizophrenen Formenkreis')]\n",
      "TP\n",
      "[]\n",
      "[(2, 18, 'Diagnosis_or_Pathology', 'Selbstschädigung')]\n",
      "FN\n",
      "[(2, 16, 'Diagnostic', 'Blutungsanamie')]\n",
      "[(2, 16, 'Diagnosis_or_Pathology', 'Blutungsanamie')]\n",
      "LE\n",
      "[(2, 14, 'Diagnosis_or_Pathology', 'Hypokaliämie')]\n",
      "[(2, 14, 'Diagnosis_or_Pathology', 'Hypokaliämie')]\n",
      "TP\n",
      "[(2, 24, 'Diagnosis_or_Pathology', 'Arterieller Hypertonus')]\n",
      "[(2, 24, 'Diagnosis_or_Pathology', 'Arterieller Hypertonus')]\n",
      "TP\n",
      "[(2, 31, 'Diagnosis_or_Pathology', 'Symptomatisches Anfallsleiden')]\n",
      "[(2, 31, 'Diagnosis_or_Pathology', 'Symptomatisches Anfallsleiden')]\n",
      "TP\n",
      "[(2, 25, 'Diagnosis_or_Pathology', 'St.p.  Aneurysmablutung')]\n",
      "[(2, 25, 'Other_Finding', 'St.p.  Aneurysmablutung')]\n",
      "LE\n",
      "[(2, 30, 'Diagnosis_or_Pathology', 'Passagerer Diabetes mellitus')]\n",
      "[(2, 30, 'Diagnosis_or_Pathology', 'Passagerer Diabetes mellitus')]\n",
      "TP\n",
      "[(12, 28, 'Diagnosis_or_Pathology', 'Handreplantaticm')]\n",
      "[(12, 28, 'Therapeutic', 'Handreplantaticm')]\n",
      "LE\n",
      "[]\n",
      "[(13, 21, 'Therapeutic', 'Revision')]\n",
      "FN\n",
      "[(12, 37, 'Therapeutic', 'Nekrosektomie Kopf / Hals')]\n",
      "[(12, 37, 'Therapeutic', 'Nekrosektomie Kopf / Hals')]\n",
      "TP\n",
      "[(12, 22, 'Therapeutic', 'Jet-Lavage'), (24, 35, 'Therapeutic', 'Debridement'), (40, 63, 'Therapeutic', 'VAG Wechsel linken Hand')]\n",
      "[(12, 63, 'Therapeutic', 'Jet-Lavage, Debridement und VAG Wechsel linken Hand')]\n",
      "BE\n",
      "[(12, 22, 'Therapeutic', 'Jet-Lavage'), (24, 35, 'Therapeutic', 'Debridement'), (40, 62, 'Therapeutic', 'VAG-Wechsel linke Hand')]\n",
      "[(12, 62, 'Therapeutic', 'Jet-Lavage, Debridement und VAG-Wechsel linke Hand')]\n",
      "BE\n",
      "[(11, 58, 'Therapeutic', 'Nachdebridement am Kopf, VAG-Wechsel linke Hand')]\n",
      "[(11, 34, 'Therapeutic', 'Nachdebridement am Kopf'), (36, 58, 'Therapeutic', 'VAG-Wechsel linke Hand')]\n",
      "BE\n",
      "[(12, 81, 'Therapeutic', 'Weicliteildeckung mit Spalthautsheet und Keratinozytensprühsuspension')]\n",
      "[(12, 81, 'Therapeutic', 'Weicliteildeckung mit Spalthautsheet und Keratinozytensprühsuspension')]\n",
      "TP\n",
      "[(12, 23, 'Therapeutic', 'Debridement'), (25, 63, 'Therapeutic', 'Jet Lavage Lind VAC Wechsel linke Hand')]\n",
      "[(12, 63, 'Therapeutic', 'Debridement, Jet Lavage Lind VAC Wechsel linke Hand')]\n",
      "BE\n",
      "[]\n",
      "[(11, 41, 'Therapeutic', 'Restdefektdeckung Kopf u. Hals')]\n",
      "FN\n",
      "[]\n",
      "[(43, 49, 'Therapeutic', 'Sheets')]\n",
      "FN\n",
      "[]\n",
      "[(52, 76, 'Therapeutic', 'Defektdeckung Handrücken')]\n",
      "FN\n",
      "[(11, 29, 'Therapeutic', 'K-Draht-Entfernung')]\n",
      "[(11, 44, 'Therapeutic', 'K-Draht-Entfernung Ii. Handgelenk')]\n",
      "BE\n",
      "[(46, 61, 'Diagnosis_or_Pathology', 'Wahnvorstellung')]\n",
      "[(46, 61, 'Diagnosis_or_Pathology', 'Wahnvorstellung')]\n",
      "TP\n",
      "[]\n",
      "[(98, 123, 'Diagnosis_or_Pathology', 'Ii. Hand selbst amputiert')]\n",
      "FN\n",
      "[]\n",
      "[(137, 157, 'Diagnosis_or_Pathology', 'Haupthaar angezündet')]\n",
      "FN\n",
      "[]\n",
      "[(309, 325, 'Diagnosis_or_Pathology', 'abgetrennte Hand')]\n",
      "FN\n",
      "[(459, 495, 'Therapeutic', 'komplette Amputation der linken Hand')]\n",
      "[(459, 495, 'Diagnosis_or_Pathology', 'komplette Amputation der linken Hand')]\n",
      "LE\n",
      "[]\n",
      "[(561, 588, 'Diagnosis_or_Pathology', 'll bis Ill gradig verbrannt')]\n",
      "FN\n",
      "[(594, 601, 'Diagnostic', 'Kopf-CT')]\n",
      "[(594, 601, 'Diagnostic', 'Kopf-CT')]\n",
      "TP\n",
      "[(616, 662, 'Other_Finding', 'Nachweis frischer intrakranieller Traumafolgen')]\n",
      "[(616, 662, 'Diagnosis_or_Pathology', 'Nachweis frischer intrakranieller Traumafolgen')]\n",
      "LE\n",
      "[]\n",
      "[(667, 708, 'Other_Finding', 'Zustand nach osteoplastischer Trepanation')]\n",
      "FN\n",
      "[]\n",
      "[(680, 768, 'Therapeutic', 'osteoplastischer Trepanation der hinteren Schädelgrube links sowie rechts frontotemporal')]\n",
      "FN\n",
      "[]\n",
      "[(773, 902, 'Therapeutic', 'nachweisbare  Gefa-Clips links infratentoriell im Bereich der linken A. vertebralis sowie im Bereich der A . cerebri media rechts')]\n",
      "FN\n",
      "[]\n",
      "[(88, 105, 'Therapeutic', 'operativ versorgt')]\n",
      "FN\n",
      "[(132, 166, 'Therapeutic', 'die ganze Nacht dauerndem Eingriff')]\n",
      "[(158, 166, 'Therapeutic', 'Eingriff')]\n",
      "BE\n",
      "[]\n",
      "[(167, 178, 'Therapeutic', 'replantiert')]\n",
      "FN\n",
      "[(191, 215, 'Diagnosis_or_Pathology', 'streckseitiger WT-Defekt')]\n",
      "[(191, 215, 'Diagnosis_or_Pathology', 'streckseitiger WT-Defekt')]\n",
      "TP\n",
      "[(235, 243, 'Clinical_Drug', 'Vacuseal')]\n",
      "[(222, 251, 'Therapeutic', 'temporär mit Vacuseal gedeckt')]\n",
      "LBE\n",
      "[(257, 275, 'Diagnosis_or_Pathology', 'verbrannten Areale')]\n",
      "[(257, 275, 'Diagnosis_or_Pathology', 'verbrannten Areale')]\n",
      "TP\n",
      "[(309, 329, 'Therapeutic', 'Lavaseptgelverbänden')]\n",
      "[(292, 338, 'Therapeutic', 'antiseptisch mit Lavaseptgelverbänden versorgt')]\n",
      "BE\n",
      "[]\n",
      "[(371, 381, 'Therapeutic', 'intuobiert')]\n",
      "FN\n",
      "[]\n",
      "[(386, 393, 'Therapeutic', 'beatmet')]\n",
      "FN\n",
      "[(422, 445, 'Diagnosis_or_Pathology', 'Brandverletztenzentrums')]\n",
      "[(422, 445, 'Diagnosis_or_Pathology', 'Brandverletztenzentrums')]\n",
      "TP\n",
      "[(28, 88, 'Therapeutic', 'Wundrevision mil Vacusealwechsel. Das Verbandsregime am Kopf')]\n",
      "[(28, 60, 'Therapeutic', 'Wundrevision mil Vacusealwechsel'), (66, 88, 'Therapeutic', 'Verbandsregime am Kopf')]\n",
      "BE\n",
      "[(103, 144, 'Therapeutic', 'enzymatisches Debridement mit lruxolsalbe')]\n",
      "[(103, 144, 'Therapeutic', 'enzymatisches Debridement mit lruxolsalbe')]\n",
      "TP\n",
      "[]\n",
      "[(133, 144, 'Clinical_Drug', 'lruxolsalbe')]\n",
      "FN\n",
      "[(161, 191, 'Therapeutic', 'erster debridierender Eingriff')]\n",
      "[(168, 191, 'Therapeutic', 'debridierender Eingriff')]\n",
      "BE\n",
      "[]\n",
      "[(4, 24, 'Therapeutic', 'erneutes Debridement')]\n",
      "FN\n",
      "[(39, 74, 'Diagnosis_or_Pathology', 'weitgehend  sauberen Wunden am Kopf')]\n",
      "[(39, 74, 'Diagnosis_or_Pathology', 'weitgehend  sauberen Wunden am Kopf')]\n",
      "TP\n",
      "[(125, 179, 'Therapeutic', 'Defektdeckung am Kopf mit Keratinozytensprühsuspension'), (184, 224, 'Therapeutic', 'Spalthautsheets vorn linken Überschenkel')]\n",
      "[(125, 224, 'Therapeutic', 'Defektdeckung am Kopf mit Keratinozytensprühsuspension und Spalthautsheets vorn linken Überschenkel')]\n",
      "BE\n",
      "[(260, 300, 'Other_Finding', 'problemlose Einheilung der Transplantate')]\n",
      "[(260, 300, 'Other_Finding', 'problemlose Einheilung der Transplantate')]\n",
      "TP\n",
      "[]\n",
      "[(287, 300, 'Therapeutic', 'Transplantate')]\n",
      "FN\n",
      "[]\n",
      "[(419, 437, 'Therapeutic', 'nachtransplantiert')]\n",
      "FN\n",
      "[(498, 514, 'Diagnosis_or_Pathology', 'kleinere Defekte')]\n",
      "[(490, 524, 'Other_Finding', 'bis auf kleinere Defekte abgeheilt')]\n",
      "LBE\n",
      "[]\n",
      "[(530, 545, 'Other_Finding', 'Entnahmestellen')]\n",
      "FN\n",
      "[]\n",
      "[(551, 574, 'Other_Finding', 'gänzlich reepithelisert')]\n",
      "FN\n",
      "[(610, 637, 'Diagnosis_or_Pathology', 'multiple alopezische Areale')]\n",
      "[(610, 637, 'Diagnosis_or_Pathology', 'multiple alopezische Areale')]\n",
      "TP\n",
      "[(662, 693, 'Diagnosis_or_Pathology', 'drittgradiger Verbrennungstiefe')]\n",
      "[(654, 693, 'Diagnosis_or_Pathology', 'initial drittgradiger Verbrennungstiefe')]\n",
      "BE\n",
      "[(55, 116, 'Diagnosis_or_Pathology', 'mittlerweile sauber granulierenden Defektes am II. Handrücken')]\n",
      "[(37, 139, 'Therapeutic', 'Defektdeckung des mittlerweile sauber granulierenden Defektes am II. Handrücken mittels Spalthautsheet')]\n",
      "LBE\n",
      "[(4, 38, 'Diagnosis_or_Pathology', 'temporare Arthrodese im Handgelenk')]\n",
      "[(4, 38, 'Therapeutic', 'temporare Arthrodese im Handgelenk')]\n",
      "LE\n",
      "[]\n",
      "[(59, 67, 'Therapeutic', 'entfernt')]\n",
      "FN\n",
      "[(69, 76, 'Therapeutic', 'Physio-'), (82, 111, 'Therapeutic', 'ergotherapeutische Behandlung')]\n",
      "[(69, 111, 'Therapeutic', 'Physio- bzw. ergotherapeutische Behandlung')]\n",
      "BE\n",
      "[]\n",
      "[(118, 141, 'Therapeutic', 'begleitend durchgeführt')]\n",
      "FN\n",
      "[(0, 40, 'Diagnosis_or_Pathology', 'Intensivmedizinischer  Krankheitsverlauf')]\n",
      "[(23, 40, 'Diagnosis_or_Pathology', 'Krankheitsverlauf')]\n",
      "BE\n",
      "[]\n",
      "[(64, 76, 'Diagnostic', 'kontrolliert')]\n",
      "FN\n",
      "[(108, 135, 'Diagnosis_or_Pathology', 'reduzierter Analgosedierung')]\n",
      "[(108, 135, 'Therapeutic', 'reduzierter Analgosedierung')]\n",
      "LE\n",
      "[]\n",
      "[(136, 166, 'Therapeutic', 'augmentiert maschinell beatmet')]\n",
      "FN\n",
      "[(172, 197, 'Therapeutic', 'FIüssigkeitseinlagerungen')]\n",
      "[(172, 219, 'Diagnosis_or_Pathology', 'FIüssigkeitseinlagerungen pulmonal und peripher')]\n",
      "LBE\n",
      "[(230, 271, 'Therapeutic', 'FIüssigkeitstherapie nach Baxter-Parkland')]\n",
      "[(230, 278, 'Therapeutic', 'FIüssigkeitstherapie nach Baxter-Parkland Formel')]\n",
      "BE\n",
      "[(301, 332, 'Therapeutic', 'forcierte Diurese mit Furosemid')]\n",
      "[(279, 332, 'Therapeutic', 'induzierten wir eine  forcierte Diurese mit Furosemid')]\n",
      "BE\n",
      "[]\n",
      "[(323, 332, 'Clinical_Drug', 'Furosemid')]\n",
      "FN\n",
      "[(352, 364, 'Therapeutic', 'Oxygenierung')]\n",
      "[(352, 386, 'Other_Finding', 'Oxygenierung im Verlauf suffizient')]\n",
      "LBE\n",
      "[(392, 418, 'Therapeutic', 'Entwöhnung vom  Respirator')]\n",
      "[(392, 418, 'Therapeutic', 'Entwöhnung vom  Respirator')]\n",
      "TP\n",
      "[(474, 497, 'Other_Finding', 'intakten Schutzreflexen')]\n",
      "[(474, 497, 'Other_Finding', 'intakten Schutzreflexen')]\n",
      "TP\n",
      "[]\n",
      "[(498, 507, 'Therapeutic', 'extubiert')]\n",
      "FN\n",
      "[(541, 566, 'Other_Finding', 'suffiziente Spontanatmung')]\n",
      "[(541, 566, 'Other_Finding', 'suffiziente Spontanatmung')]\n",
      "TP\n",
      "[(578, 607, 'Other_Finding', 'peripheren Sauerstoffsättgung')]\n",
      "[(578, 631, 'Other_Finding', 'peripheren Sauerstoffsättgung von 98%  unter Raumluft')]\n",
      "BE\n",
      "[(5, 35, 'Therapeutic', 'Beendigung der Analgosedierung')]\n",
      "[(20, 35, 'Therapeutic', 'Analgosedierung')]\n",
      "BE\n",
      "[(51, 70, 'Diagnosis_or_Pathology', 'mildes Entzugsdelir')]\n",
      "[(51, 70, 'Diagnosis_or_Pathology', 'mildes Entzugsdelir')]\n",
      "TP\n",
      "[(91, 99, 'Clinical_Drug', 'Clonidin')]\n",
      "[(76, 109, 'Other_Finding', 'suffizient mit Clonidin behandlet')]\n",
      "LBE\n",
      "[]\n",
      "[(91, 99, 'Clinical_Drug', 'Clonidin')]\n",
      "FN\n",
      "[(130, 171, 'Diagnosis_or_Pathology', 'intermittierende psychotische Symptomatik')]\n",
      "[(130, 171, 'Diagnosis_or_Pathology', 'intermittierende psychotische Symptomatik')]\n",
      "TP\n",
      "[(202, 211, 'Clinical_Drug', 'Lorazepam'), (213, 219, 'Clinical_Drug', 'Tavor®'), (225, 236, 'Clinical_Drug', 'Haloperidol')]\n",
      "[(172, 236, 'Therapeutic', 'therapierten wir zunächst mit Lorazepam (Tavor®) und Haloperidol')]\n",
      "LBE\n",
      "[]\n",
      "[(202, 211, 'Clinical_Drug', 'Lorazepam')]\n",
      "FN\n",
      "[(238, 245, 'Clinical_Drug', 'Haldol®')]\n",
      "[(213, 218, 'Clinical_Drug', 'Tavor'), (225, 236, 'Clinical_Drug', 'Haloperidol'), (238, 244, 'Clinical_Drug', 'Haldol')]\n",
      "BE\n",
      "[(280, 290, 'Clinical_Drug', 'Medikation')]\n",
      "[(280, 290, 'Therapeutic', 'Medikation')]\n",
      "LE\n",
      "[(312, 322, 'Clinical_Drug', 'Risperidon'), (324, 334, 'Clinical_Drug', 'Risperdal®')]\n",
      "[(291, 343, 'Therapeutic', 'einschleichend durch Risperidon (Risperdal®) ersetzt')]\n",
      "LBE\n",
      "[]\n",
      "[(312, 322, 'Clinical_Drug', 'Risperidon')]\n",
      "FN\n",
      "[]\n",
      "[(324, 333, 'Clinical_Drug', 'Risperdal')]\n",
      "FN\n",
      "[(349, 409, 'Therapeutic', 'vorbestehende antiepileptische Medikation mit Levothiracetam')]\n",
      "[(349, 409, 'Therapeutic', 'vorbestehende antiepileptische Medikation mit Levothiracetam')]\n",
      "TP\n",
      "[]\n",
      "[(395, 409, 'Clinical_Drug', 'Levothiracetam')]\n",
      "FN\n",
      "[(411, 418, 'Clinical_Drug', 'Keppra®')]\n",
      "[(411, 417, 'Clinical_Drug', 'Keppra')]\n",
      "BE\n",
      "[(461, 484, 'Other_Finding', 'psychiatrischen Verlauf')]\n",
      "[]\n",
      "FP\n",
      "[(41, 68, 'Other_Finding', 'positive FIüssigkeitsbilanz')]\n",
      "[(41, 68, 'Other_Finding', 'positive FIüssigkeitsbilanz')]\n",
      "TP\n",
      "[(76, 83, 'Therapeutic', 'Diurese'), (94, 114, 'Therapeutic', 'FIüssigkeitstherapie')]\n",
      "[(76, 197, 'Other_Finding', 'Diurese war unter FIüssigkeitstherapie unter Anlehnung an die Baxter-Parklandformel qualitativ und quantitativ suffizient')]\n",
      "LBE\n",
      "[]\n",
      "[(94, 159, 'Therapeutic', 'FIüssigkeitstherapie unter Anlehnung an die Baxter-Parklandformel')]\n",
      "FN\n",
      "[(242, 278, 'Therapeutic', 'antiödematöse Therapie mit Furosemid')]\n",
      "[(242, 278, 'Therapeutic', 'antiödematöse Therapie mit Furosemid')]\n",
      "TP\n",
      "[]\n",
      "[(269, 278, 'Clinical_Drug', 'Furosemid')]\n",
      "FN\n",
      "[(294, 313, 'Other_Finding', 'Retentionsparameter'), (331, 342, 'Other_Finding', 'Normbereich')]\n",
      "[(294, 342, 'Other_Finding', 'Retentionsparameter lagen allzeit im Normbereich')]\n",
      "BE\n",
      "[(6, 31, 'Therapeutic', 'adäquater Volumentherapie')]\n",
      "[(6, 31, 'Therapeutic', 'adäquater Volumentherapie')]\n",
      "TP\n",
      "[]\n",
      "[(82, 102, 'Other_Finding', 'hämodynamisch stabil')]\n",
      "FN\n",
      "[(109, 139, 'Therapeutic', 'Beendigung der Analgosedierung')]\n",
      "[(124, 139, 'Therapeutic', 'Analgosedierung')]\n",
      "BE\n",
      "[(150, 172, 'Diagnosis_or_Pathology', 'arteriellem Hypertonus')]\n",
      "[(150, 172, 'Diagnosis_or_Pathology', 'arteriellem Hypertonus')]\n",
      "TP\n",
      "[(178, 249, 'Therapeutic', 'antihypertensive Therapie mit Metoprolol (Beloc zok mite®) und Ramipril'), (251, 257, 'Clinical_Drug', 'Delix®')]\n",
      "[(178, 267, 'Therapeutic', 'antihypertensive Therapie mit Metoprolol (Beloc zok mite®) und Ramipril (Delix®) begonnen')]\n",
      "LBE\n",
      "[]\n",
      "[(208, 218, 'Clinical_Drug', 'Metoprolol')]\n",
      "FN\n",
      "[]\n",
      "[(220, 234, 'Clinical_Drug', 'Beloc zok mite')]\n",
      "FN\n",
      "[]\n",
      "[(241, 249, 'Clinical_Drug', 'Ramipril')]\n",
      "FN\n",
      "[]\n",
      "[(251, 256, 'Clinical_Drug', 'Delix')]\n",
      "FN\n",
      "[(289, 328, 'Other_Finding', 'zufriedenstellende Blutdruckeinstellung')]\n",
      "[(289, 328, 'Other_Finding', 'zufriedenstellende Blutdruckeinstellung')]\n",
      "TP\n",
      "[(6, 21, 'Therapeutic', 'Analgosedierung')]\n",
      "[(6, 21, 'Therapeutic', 'Analgosedierung')]\n",
      "TP\n",
      "[(39, 89, 'Therapeutic', 'enterale Ernährung über die einliegende Magensonde')]\n",
      "[(22, 89, 'Therapeutic', 'begannen wir die enterale Ernährung über die einliegende Magensonde')]\n",
      "BE\n",
      "[(96, 107, 'Therapeutic', 'Stimulation')]\n",
      "[(91, 157, 'Other_Finding', 'Nach Stimulation führte Frau Albers am 26.3.2029 zum ersten Mal ab')]\n",
      "LBE\n",
      "[]\n",
      "[(96, 107, 'Therapeutic', 'Stimulation')]\n",
      "FN\n",
      "[(164, 174, 'Therapeutic', 'Extubation')]\n",
      "[(164, 174, 'Therapeutic', 'Extubation')]\n",
      "TP\n",
      "[(186, 202, 'Therapeutic', 'orale Kostaufbau')]\n",
      "[(186, 202, 'Therapeutic', 'orale Kostaufbau')]\n",
      "TP\n",
      "[(0, 23, 'Diagnosis_or_Pathology', 'Psychiatrischer Verlauf')]\n",
      "[]\n",
      "FP\n",
      "[]\n",
      "[(0, 17, 'Diagnostic', 'Fremdanamnestisch')]\n",
      "FN\n",
      "[]\n",
      "[(47, 68, 'Therapeutic', 'ambulant behandelnden')]\n",
      "FN\n",
      "[(96, 110, 'Therapeutic', 'Psychotherapie')]\n",
      "[(96, 110, 'Therapeutic', 'Psychotherapie')]\n",
      "TP\n",
      "[]\n",
      "[(174, 190, 'Diagnosis_or_Pathology', 'akut psychotisch')]\n",
      "FN\n",
      "[]\n",
      "[(225, 271, 'Therapeutic', 'antipsychotische Erhaltungsmedikation von 2 mg')]\n",
      "FN\n",
      "[]\n",
      "[(287, 307, 'Therapeutic', 'ambulant behandelnde')]\n",
      "FN\n",
      "[]\n",
      "[(398, 410, 'Therapeutic', 'behandelnden')]\n",
      "FN\n",
      "[(456, 479, 'Therapeutic', 'medikamentösen Therapie')]\n",
      "[(456, 479, 'Therapeutic', 'medikamentösen Therapie')]\n",
      "TP\n",
      "[(508, 524, 'Diagnosis_or_Pathology', 'Selbstverletzung')]\n",
      "[(508, 524, 'Diagnosis_or_Pathology', 'Selbstverletzung')]\n",
      "TP\n",
      "[(637, 670, 'Therapeutic', 'freiwilligen stationären Aufnahme')]\n",
      "[]\n",
      "FP\n",
      "[(698, 718, 'Other_Finding', 'Unterbringungsgründe')]\n",
      "[(698, 718, 'Therapeutic', 'Unterbringungsgründe')]\n",
      "LE\n",
      "[]\n",
      "[(738, 781, 'Therapeutic', 'Vorstellung bei der ambulanten Psychiaterin')]\n",
      "FN\n",
      "[(882, 894, 'Diagnosis_or_Pathology', 'Verletzungen')]\n",
      "[(882, 894, 'Diagnosis_or_Pathology', 'Verletzungen')]\n",
      "TP\n",
      "[(21, 45, 'Diagnosis_or_Pathology', 'erhebliche Denkstörungen')]\n",
      "[(21, 45, 'Diagnosis_or_Pathology', 'erhebliche Denkstörungen')]\n",
      "TP\n",
      "[(64, 75, 'Other_Finding', 'Wahnerleben')]\n",
      "[(64, 92, 'Diagnosis_or_Pathology', 'Wahnerleben an, Gott zu sein')]\n",
      "LBE\n",
      "[]\n",
      "[(129, 172, 'Diagnosis_or_Pathology', 'linke Hand mit dem Brotmesser abgeschnitten')]\n",
      "FN\n",
      "[]\n",
      "[(259, 278, 'Diagnosis_or_Pathology', 'erheblich inadäquat')]\n",
      "FN\n",
      "[(365, 420, 'Therapeutic', 'Sedierung mit Tavor und nach Umstellung auf Haloperidol')]\n",
      "[(365, 384, 'Therapeutic', 'Sedierung mit Tavor'), (379, 384, 'Clinical_Drug', 'Tavor'), (394, 420, 'Therapeutic', 'Umstellung auf Haloperidol')]\n",
      "LBE\n",
      "[]\n",
      "[(409, 420, 'Clinical_Drug', 'Haloperidol')]\n",
      "FN\n",
      "[(437, 448, 'Diagnosis_or_Pathology', 'Symptomatik')]\n",
      "[(421, 448, 'Other_Finding', 'sistierte diese Symptomatik')]\n",
      "LBE\n",
      "[(476, 503, 'Clinical_Drug', 'antipsychotische Medikation')]\n",
      "[(476, 503, 'Therapeutic', 'antipsychotische Medikation')]\n",
      "LE\n",
      "[(544, 558, 'Clinical_Drug', '6 mg Risperdal')]\n",
      "[(525, 569, 'Therapeutic', 'auf eine Dosis von 6 mg Risperdal umstellten')]\n",
      "LBE\n",
      "[]\n",
      "[(544, 558, 'Clinical_Drug', '6 mg Risperdal')]\n",
      "FN\n",
      "[(575, 580, 'Clinical_Drug', 'Tavor')]\n",
      "[(575, 580, 'Clinical_Drug', 'Tavor')]\n",
      "TP\n",
      "[]\n",
      "[(589, 604, 'Therapeutic', 'auszuschleichen')]\n",
      "FN\n",
      "[]\n",
      "[(28, 50, 'Diagnosis_or_Pathology', 'weitgehend distanziert')]\n",
      "FN\n",
      "[(55, 76, 'Diagnosis_or_Pathology', 'psychotischem Erleben')]\n",
      "[(55, 76, 'Diagnosis_or_Pathology', 'psychotischem Erleben')]\n",
      "TP\n",
      "[]\n",
      "[(78, 83, 'Other_Finding', 'ruhig')]\n",
      "FN\n",
      "[]\n",
      "[(85, 99, 'Other_Finding', 'gut im Kontakt')]\n",
      "FN\n",
      "[]\n",
      "[(104, 113, 'Other_Finding', 'bedauerte')]\n",
      "FN\n",
      "[(175, 210, 'Therapeutic', 'stationäre psychiatrische Verlegung')]\n",
      "[]\n",
      "FP\n",
      "[]\n",
      "[(245, 267, 'Therapeutic', 'motivierenden Gespräch')]\n",
      "FN\n",
      "[(372, 380, 'Therapeutic', 'Therapie')]\n",
      "[(372, 380, 'Therapeutic', 'Therapie')]\n",
      "TP\n",
      "[(478, 489, 'Clinical_Drug', 'Medikamente')]\n",
      "[(478, 489, 'Clinical_Drug', 'Medikamente')]\n",
      "TP\n",
      "[]\n",
      "[(494, 500, 'Diagnosis_or_Pathology', 'Affekt')]\n",
      "FN\n",
      "[]\n",
      "[(526, 573, 'Diagnosis_or_Pathology', 'distanziert zur Dramatik ihrer Selbstverletzung')]\n",
      "FN\n",
      "[]\n",
      "[(36, 104, 'Therapeutic', 'Belastungserprobung unter zunehmender Konfrontation milder Außenwelt')]\n",
      "FN\n",
      "[(125, 167, 'Therapeutic', 'stationäre intensivmedizinische Behandlung')]\n",
      "[(125, 167, 'Therapeutic', 'stationäre intensivmedizinische Behandlung')]\n",
      "TP\n",
      "[(198, 255, 'Therapeutic', 'sozialpsychiatrische Einbettung in ein ambulantes Setting')]\n",
      "[]\n",
      "FP\n",
      "[(268, 293, 'Diagnosis_or_Pathology', 'schweren Selbstverletzung')]\n",
      "[(268, 293, 'Diagnosis_or_Pathology', 'schweren Selbstverletzung')]\n",
      "TP\n",
      "[(315, 377, 'Therapeutic', 'therapeutische Begleitung bei der Bewältigung der Folgeschaden')]\n",
      "[(315, 340, 'Therapeutic', 'therapeutische Begleitung'), (349, 377, 'Therapeutic', 'Bewältigung der Folgeschaden')]\n",
      "BE\n",
      "[]\n",
      "[(365, 387, 'Diagnosis_or_Pathology', 'Folgeschaden im Alltag')]\n",
      "FN\n",
      "[(102, 142, 'Therapeutic', 'stationäre fachpsychiatrische Behandlung')]\n",
      "[(102, 142, 'Therapeutic', 'stationäre fachpsychiatrische Behandlung')]\n",
      "TP\n",
      "[(55, 77, 'Therapeutic', 'teils flexible Orthese')]\n",
      "[(55, 77, 'Therapeutic', 'teils flexible Orthese')]\n",
      "TP\n",
      "[(179, 233, 'Therapeutic', 'ergo- und physiotherapeutische Beübung der Iinken Hand')]\n",
      "[(165, 247, 'Therapeutic', '1 x tgl. eine ergo- und physiotherapeutische Beübung der Iinken Hand durchzuführen')]\n",
      "BE\n",
      "[]\n",
      "[(269, 343, 'Therapeutic', 'Fingergelenke und das Handgelenk belastungsfrei in allen Ebenen therapiert')]\n",
      "FN\n",
      "[(356, 363, 'Therapeutic', 'Übungen'), (397, 404, 'Therapeutic', 'Orthese')]\n",
      "[(356, 411, 'Therapeutic', 'Übungen sollten aktiv und passiv aus der Orthese heraus')]\n",
      "BE\n",
      "[]\n",
      "[(4, 16, 'Therapeutic', 'Transplantat')]\n",
      "FN\n",
      "[(22, 36, 'Other_Finding', 'Entnahmeareale')]\n",
      "[(22, 36, 'Therapeutic', 'Entnahmeareale')]\n",
      "LE\n",
      "[(63, 79, 'Clinical_Drug', 'fettenden Salben'), (86, 95, 'Clinical_Drug', 'Panthenol')]\n",
      "[(45, 105, 'Therapeutic', 'tgl. mehrfach mit fettenden Salben (z.B. Panthenol) gepflegt')]\n",
      "LBE\n",
      "[]\n",
      "[(63, 79, 'Clinical_Drug', 'fettenden Salben')]\n",
      "FN\n",
      "[]\n",
      "[(86, 95, 'Clinical_Drug', 'Panthenol')]\n",
      "FN\n",
      "[(114, 125, 'Diagnosis_or_Pathology', 'Restdefekte')]\n",
      "[(114, 125, 'Diagnosis_or_Pathology', 'Restdefekte')]\n",
      "TP\n",
      "[(160, 178, 'Clinical_Drug', 'Betaisodona – Lsg.')]\n",
      "[(156, 188, 'Therapeutic', 'mit Betaisodona – Lsg. behandelt')]\n",
      "LBE\n",
      "[(78, 109, 'Therapeutic', 'mehrere korrigierende Eingriffe')]\n",
      "[(86, 109, 'Therapeutic', 'korrigierende Eingriffe')]\n",
      "BE\n",
      "[(155, 191, 'Therapeutic', 'operative Sehnen- und Gelenklösungen')]\n",
      "[(155, 191, 'Therapeutic', 'operative Sehnen- und Gelenklösungen')]\n",
      "TP\n",
      "[(196, 235, 'Therapeutic', 'Verbesserung der Beweglichkeit der Hand')]\n",
      "[(196, 226, 'Therapeutic', 'Verbesserung der Beweglichkeit'), (213, 235, 'Other_Finding', 'Beweglichkeit der Hand')]\n",
      "LBE\n",
      "[(242, 282, 'Therapeutic', 'Korrektureingriffe im Bereich der Narben')]\n",
      "[(242, 282, 'Therapeutic', 'Korrektureingriffe im Bereich der Narben')]\n",
      "TP\n",
      "[]\n",
      "[(276, 345, 'Diagnosis_or_Pathology', 'Narben insbesondere im Kopf-/Halsbereich und der haarlosen Kopfareale')]\n",
      "FN\n",
      "[(0, 11, 'Clinical_Drug', 'Medikamente')]\n",
      "[(0, 11, 'Clinical_Drug', 'Medikamente')]\n",
      "TP\n",
      "[(0, 28, 'Clinical_Drug', 'Enoxaparin (Clexane®) 0,4 ml')]\n",
      "[(0, 28, 'Clinical_Drug', 'Enoxaparin (Clexane®) 0,4 ml')]\n",
      "TP\n",
      "[(0, 29, 'Clinical_Drug', 'Pantoprazol (Pantozol®) 40 mg')]\n",
      "[(0, 29, 'Clinical_Drug', 'Pantoprazol (Pantozol®) 40 mg')]\n",
      "TP\n",
      "[(0, 24, 'Clinical_Drug', 'Ramipril (Detix®) 2.5 mg')]\n",
      "[(0, 24, 'Clinical_Drug', 'Ramipril (Detix®) 2.5 mg')]\n",
      "TP\n",
      "[(0, 10, 'Clinical_Drug', 'Metoprolol')]\n",
      "[(0, 36, 'Clinical_Drug', 'Metoprolol (Beloc zok mite®) 47,5 mg')]\n",
      "BE\n",
      "[(0, 33, 'Clinical_Drug', 'Levothir c.etarn (Keppra®) 1500mg')]\n",
      "[(0, 33, 'Clinical_Drug', 'Levothir c.etarn (Keppra®) 1500mg')]\n",
      "TP\n",
      "[(0, 28, 'Clinical_Drug', 'Risperidon {Risperdaf®) 2 mg')]\n",
      "[(0, 28, 'Clinical_Drug', 'Risperidon {Risperdaf®) 2 mg')]\n",
      "TP\n",
      "[(0, 16, 'Clinical_Drug', 'Ibuprofen .600mg')]\n",
      "[(0, 16, 'Clinical_Drug', 'Ibuprofen .600mg')]\n",
      "TP\n",
      "[(0, 7, 'Clinical_Drug', 'Thiamin')]\n",
      "[(0, 52, 'Clinical_Drug', 'Thiamin (Vil. B1) 100 mg + Pyridoxin (Vit. 8) 100 mg')]\n",
      "BE\n",
      "[(54, 72, 'Clinical_Drug', 'Neurorathiopharrn®')]\n",
      "[(54, 71, 'Clinical_Drug', 'Neurorathiopharrn')]\n",
      "BE\n",
      "[(10, 33, 'Clinical_Drug', 'bioaquivalente Generika')]\n",
      "[(10, 33, 'Clinical_Drug', 'bioaquivalente Generika')]\n",
      "TP\n",
      "[]\n",
      "[(25, 53, 'Therapeutic', 'Generika (weiter-) verordnet')]\n",
      "FN\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from spacy import util\n",
    "\n",
    "sys.path.append('../experiments')\n",
    "from error_analysis import _ner_error_analyis\n",
    "\n",
    "# Extract paths to configuration files\n",
    "CONFIG = {\n",
    "    'json_path':'../GraSSco/grassco_anno_2023-01-05_0021/json/fine/long/test.json',\n",
    "    'json_path_new': '../GraSSco/grascco_hpi_anno_2023_02_08/annotations/json/fine/long/all_short.json',\n",
    "    'ggponc_test_set': '../test/json/json/fine/short/test.json'\n",
    "}\n",
    "\n",
    "def find_annotated_entities(sentence, document):\n",
    "    s_offsets = sentence['offsets']\n",
    "    return [entity for entity in document['entities']\n",
    "            if entity and\n",
    "            (entity['offsets'][0][0] >= s_offsets[0][0] and\n",
    "            entity['offsets'][0][1] <= s_offsets[0][1])]\n",
    "            \n",
    "# Use descriptive variable names and split long lines\n",
    "def compare_findings(pred_entities, truth_entities, sentence):\n",
    "    sentence_delta = sentence['offsets'][0][0]\n",
    "    pred_spans = []\n",
    "    gt_spans = []\n",
    "    filtered_pred_entities = util.filter_spans(pred_entities.spans['snomed'])\n",
    "\n",
    "    for p in filtered_pred_entities:\n",
    "        pred_spans.append({\n",
    "            'start': p.start_char,\n",
    "            'end': p.end_char,\n",
    "            'entity_group': p.label_,\n",
    "            'word': p.text\n",
    "        })\n",
    "\n",
    "    for t in truth_entities:\n",
    "        gt_spans.append({\n",
    "            'start': t['offsets'][0][0] - sentence_delta,\n",
    "            'end': t['offsets'][0][1] - sentence_delta,\n",
    "            'entity_group': t['type'],\n",
    "            'word': t['text'][0]\n",
    "        })\n",
    "\n",
    "    errors = _ner_error_analyis(pred_spans, gt_spans)\n",
    "\n",
    "    true_positive_count = sum(1 for e in pred_entities.spans['snomed']\n",
    "                              if any(e.label_ == t['type'] and\n",
    "                                     e.start_char == t['offsets'][0][0] - sentence_delta and\n",
    "                                     e.end_char == t['offsets'][0][1] - sentence_delta\n",
    "                                     for t in truth_entities))\n",
    "\n",
    "    return true_positive_count, len(filtered_pred_entities), len(truth_entities), errors\n",
    "\n",
    "\n",
    "tp = e_count = t_count = 0\n",
    "errors = []\n",
    "\n",
    "\n",
    "with open(CONFIG['json_path_new']) as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "\n",
    "for document in data:\n",
    "    print(\"Current document_id: \"+document['document_id'])\n",
    "    for i, nlp_findings in enumerate(nlp.pipe([d['text'] for d in document['passages'] if 'text' in d]\n",
    ", disable=[\"tok2vec\", \"tagger\", \"attribute_ruler\",\"lemmatizer\"])): # use pipe, disable_components in pipe (standard NER tagger etc.) - \"parser\" is helpful (10 matches less when not used)\n",
    "        manual_findings = find_annotated_entities(document['passages'][i], document)\n",
    "        tp_i, e_count_i, t_count_i, errors_i = compare_findings(nlp_findings, manual_findings, document['passages'][i])\n",
    "        tp += tp_i\n",
    "        e_count += e_count_i\n",
    "        t_count += t_count_i\n",
    "        errors.extend(errors_i)\n",
    "        \n",
    "#print TP by error_analysis function\n",
    "print(len([e for e in errors if e['category'] == 'true_positive']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "473c097d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Entities: 195\n",
      "Predicted Entities: 187\n",
      "True Positives: 87\n",
      "False Positives: 100\n",
      "False Negatives: 108\n",
      "Precision: 0.47\n",
      "Recall: 0.45\n",
      "F1 Score: 0.46\n"
     ]
    }
   ],
   "source": [
    "fn = t_count-tp\n",
    "fp = e_count-tp\n",
    "\n",
    "print(f'Actual Entities: {t_count}')\n",
    "print(f'Predicted Entities: {e_count}')\n",
    "\n",
    "print(f'True Positives: {tp}')\n",
    "print(f'False Positives: {fp}')\n",
    "print(f'False Negatives: {fn}')\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1_score:.2f}')\n",
    "\n",
    "\n",
    "#result for fine/long\n",
    "\n",
    "#Actual Entities: 6194\n",
    "#Predicted Entities: 5171\n",
    "#True Positives: 2584\n",
    "#False Positives: 2587\n",
    "#False Negatives: 3610\n",
    "#Precision: 0.50\n",
    "#Recall: 0.42\n",
    "#F1 Score: 0.45\n",
    "\n",
    "#result for fine/short\n",
    "#Actual Entities: 7201\n",
    "#Predicted Entities: 5171\n",
    "#True Positives: 1737\n",
    "#False Positives: 3434\n",
    "#False Negatives: 5464\n",
    "#Precision: 0.34\n",
    "#Recall: 0.24\n",
    "#F1 Score: 0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df2f582c-bcf0-466c-99bb-6a06328cbb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versagen einer Behandlung Diagnosis_or_Pathology\n",
      "Behandlung mit Oxaliplatin und Irinotecan Therapeutic\n",
      "Oxaliplatin Clinical_Drug\n",
      "Irinotecan Clinical_Drug\n"
     ]
    }
   ],
   "source": [
    "for s in sorted(list(doc.spans['snomed']), key=lambda s: s.start):\n",
    "    print(s, s.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9478a3e2",
   "metadata": {},
   "source": [
    "## Document Based Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all documents... this takes up to multiple minutes hang tight!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "#nlp = spacy.load('../data/models/spacy')\n",
    "\n",
    "def get_string_sentences(docs):\n",
    "    return ['\\n'.join([sentence['text'] for sentence in doc['passages']]) for doc in docs]\n",
    "\n",
    "def find_annotated_entities(annotated_doc):\n",
    "    return [entity for entity in annotated_doc['entities'] if entity]\n",
    "\n",
    "def compare_findings(predicted_findings, truth_entities):\n",
    "    predicted_findings = sorted(predicted_findings, key=lambda x: x.start_char)\n",
    "    truth_set = set(t['offsets'][0][0] for t in truth_entities)\n",
    "    tp, e_count, t_count = 0, len(predicted_findings), len(truth_entities)\n",
    "    for e in predicted_findings:\n",
    "        if e.start_char in truth_set:\n",
    "            t = next(t for t in truth_entities if t['offsets'][0][0] == e.start_char)\n",
    "            if e.label_ == t['type'] and e.start_char == t['offsets'][0][0] and e.end_char == t['offsets'][0][1]:\n",
    "                tp += 1\n",
    "    return tp, e_count, t_count\n",
    "\n",
    "tp, e_count, t_count = 0, 0, 0\n",
    "\n",
    "json_path = \"/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/GraSSco/grassco_anno_2023-01-05_0021/json/fine/long/test.json\"\n",
    "json_path_new = \"/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/GraSSco/grascco_hpi_anno_2023_02_08/annotations/json/fine/long/all_short.json\"\n",
    "\n",
    "with open(json_path_new) as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "print(\"Processing all documents... this takes up to multiple minutes hang tight!\")\n",
    "\n",
    "# use pipe, disable_components in pipe (standard NER tagger etc.) - \"parser\" is helpful (10 matches less when not used)\n",
    "for i, doc in enumerate(nlp.pipe(get_string_sentences(data), disable=[\"tok2vec\", \"attribute_ruler\", \"lemmatizer\"])):\n",
    "    predicted_findings = [(ent) for ent in list(doc.spans['snomed'])]\n",
    "    manual_findings = find_annotated_entities(data[i])\n",
    "    tp_i, e_count_i, t_count_i = compare_findings(predicted_findings, manual_findings)\n",
    "    tp += tp_i\n",
    "    e_count += e_count_i\n",
    "    t_count += t_count_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a36fd872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Entities: 195\n",
      "Predicted Entities: 177\n",
      "True Positives: 81\n",
      "False Positives: 96\n",
      "False Negatives: 114\n",
      "Precision: 0.46\n",
      "Recall: 0.42\n",
      "F1 Score: 0.44\n"
     ]
    }
   ],
   "source": [
    "fn = t_count-tp\n",
    "fp = e_count-tp\n",
    "\n",
    "print(f'Actual Entities: {t_count}')\n",
    "print(f'Predicted Entities: {e_count}')\n",
    "\n",
    "print(f'True Positives: {tp}')\n",
    "print(f'False Positives: {fp}')\n",
    "print(f'False Negatives: {fn}')\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1_score:.2f}')\n",
    "\n",
    "\n",
    "#sentence based result for all-short\n",
    "\n",
    "#Actual Entities: 195\n",
    "#Predicted Entities: 187\n",
    "#True Positives: 87\n",
    "#False Positives: 100\n",
    "#False Negatives: 108\n",
    "#Precision: 0.47\n",
    "#Recall: 0.45\n",
    "#F1 Score: 0.46\n",
    "\n",
    "#document based result for all-short\n",
    "\n",
    "#Actual Entities: 195\n",
    "#Predicted Entities: 177\n",
    "#True Positives: 81\n",
    "#False Positives: 96\n",
    "#False Negatives: 114\n",
    "#Precision: 0.46\n",
    "#Recall: 0.42\n",
    "#F1 Score: 0.44"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
