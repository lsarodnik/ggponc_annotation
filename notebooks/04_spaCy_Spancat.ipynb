{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dd282e9-2b3b-4319-9eb0-d60b9fd968f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prediction of overlapping spans with spaCy's SpanCategorizer\n",
    "\n",
    "**Motivation**:\n",
    "\n",
    "Annotations in GGPONC are often overlapping or nested.\n",
    "\n",
    "For instance, `Versagen einer Behandlung mit Oxaliplatin und Irinotecan`\n",
    "- is a *Finding*\n",
    "- which contains a *Therapeutic Procedure*: `Behandlung mit Oxaliplatin und Irinotecan`:\n",
    "    - which in turn contains two *Clinical Drug* names: (`Oxaliplatin` and `Irinotecan`).\n",
    "\n",
    "Standard IOB-encoded labels, and most NER implementations, can only model one label per token, so by default we consider the longest surrounding mention span only in the IOB-based / HuggingFace implementation (in this case, the *Finding*).\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "Instead of token-level labels, we use spaCy's new [SpanCategorizer](https://spacy.io/api/spancategorizer/) implementation to predict overlapping mention spans as a SpanGroup in a spaCy document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add8d33-0eaa-47f7-856d-630e54581a7b",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "See the `spacy` folder in the root directory of the project. The model configuration can be found at `configs` and training can be run through a spaCy project (see `spacy/run_training.sh`). \n",
    "\n",
    "*Note:* We have currently not optimized the many hyperparameters related to span suggestion and model training. However, performance is close to the HuggingFace models evaluated on non-nested mention spans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0cd5b1-300e-4710-a85f-e2413bb33df7",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357713b1-2285-4923-826b-f7e58178c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14a1448-ab8c-4cf7-9fd8-c728b21514f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, Span\n",
    "import snomed_spans #TODO: import needed to enable custom spaCy components, is there another way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23f22a8-c73d-4508-a756-f220d167e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('../data/models/spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d6044-d4f1-4d1d-b608-a0069d03617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"\"\"Versagen einer Behandlung mit Oxaliplatin und Irinotecan\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e211c2b5",
   "metadata": {},
   "source": [
    "### Grascco Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20601a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"\"\"6.04.2029: Nachdebridement am Kopf, VAG-Wechsel linke Hand\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f493809",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"6.04.2029: Nachdebridement am Kopf, VAG-Wechsel linke Hand\"\"Röntgen : Rippstein I : Gute Hüftkopfepiphysenkonturgebung , minimale Lateralisation , li. etwas stärker als re. , noch übergreifende Pfannendächer , Shenton-Menard-Linie nicht wesentlich unterbrochen , Pfannendachwinkel Ii. 30° , re. ebenfalls knapp 30° .\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f547e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from sklearn.metrics import f1_score\n",
    "from spacy.training import docs_to_json\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training import Corpus\n",
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "folder_raw = \"/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/grassco_project_2023-01-05_0021/source\"\n",
    "manual_annotated_file = \"/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/grassco_project_2023-01-05_0021/grassco_converted/spacy/test.spacy\"\n",
    "p = Path(r'/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/grassco_project_2023-01-05_0021/source').glob('*.txt')\n",
    "files = [x for x in p if x.is_file()]\n",
    "\n",
    "\n",
    "\n",
    "print(\"loading .spacy file ...\")\n",
    "#gold_annotation = nlp.from_disk(manual_annotated_file)\n",
    "#doc_bin = DocBin().from_disk(manual_annotated_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"working ...\")\n",
    "text = \"\"\n",
    "#text = \"\"\n",
    "for file in files:\n",
    "    text += file.read_text(encoding=\"utf-8\")\n",
    "docs = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dcaf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc_bin = DocBin().from_disk(manual_annotated_file)\n",
    "\n",
    "\n",
    "gold_docs  = list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "#scorer = Scorer()\n",
    "#scores = scorer.score(examples)\n",
    "\n",
    "#gold_annotation = spacy.Corpus.v1(manual_annotated_file, gold_preproc=True)\n",
    "docs = []\n",
    "for file in files:\n",
    "    docs.append(file.read_text(encoding=\"utf-8\"))\n",
    "print(\"Files merged...\")\n",
    "\n",
    "\n",
    "# Loop over the gold standard data\n",
    "#for gold_doc in gold_docs:\n",
    "#    ents1 = [(gold_doc.text, gold_doc.label_) for ent in gold_doc.ents]\n",
    "    # Process the text with the model\n",
    "# Compare the model's predicted annotations with the gold standard\n",
    "#for doc in docs:\n",
    "#    ents2 = [(doc.text, doc.label_) for ent in doc.ents]\n",
    "\n",
    "print(\"NLP Pipe...\")\n",
    "docs_all = nlp.pipe(gold_docs, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"],batch_size = 10)\n",
    "\n",
    "\n",
    "print(gold_docs[1])\n",
    "print(\"Building Examples...\")\n",
    "examples = []\n",
    "for  i, doc in enumerate(docs_all):\n",
    "    examples.append(Example((doc), gold_docs[i]))\n",
    "    \n",
    "\n",
    "\n",
    "scorer = Scorer(nlp)\n",
    "\n",
    "print(\"eval...\")\n",
    "scores = scorer.score(examples)\n",
    "\n",
    "\n",
    "#example_object = iter(docs)\n",
    "\n",
    "\n",
    "#examples = Example(Doc.from_docs(docs), Doc.from_docs(gold_docs))\n",
    "\n",
    "scorer.score()\n",
    "\n",
    "\n",
    "#print(\"Entities F-Score:\", scorer.scores[\"ents_f\"])\n",
    "#print(\"Entities Precision:\", scorer.scores[\"ents_p\"])\n",
    "#print(\"Entities Recall:\", scorer.scores[\"ents_r\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f72f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = Scorer()\n",
    "\n",
    "scores = scorer.score(examples)\n",
    "\n",
    "print(\"Entities F-Score:\", scorer.scores[\"ents_f\"])\n",
    "print(\"Entities Precision:\", scorer.scores[\"ents_p\"])\n",
    "print(\"Entities Recall:\", scorer.scores[\"ents_r\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a007a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence['offsets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60a0f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def find_annotated_entities(sentence, document):\n",
    "    s_offsets = sentence['offsets']\n",
    "    entity_list = []\n",
    "    for entity in document['entities']:\n",
    "        if entity is None:\n",
    "            break\n",
    "        elif(entity['offsets'][0][0] > s_offsets[0][1]): \n",
    "            break\n",
    "        elif (entity['offsets'][0][0] >=  s_offsets[0][0] and entity['offsets'][0][1] <= s_offsets[0][1]):\n",
    "            entity_list.append(entity)\n",
    "    return entity_list\n",
    "\n",
    "tp = 0\n",
    "e_count = 0\n",
    "t_count = 0\n",
    "\n",
    "def compare_findings(predi_entities, truth_entities, sentence):\n",
    "    sentence_delta = sentence['offsets'][0][0]\n",
    "    global tp, e_count, t_count\n",
    "    for e in list(predi_entities.spans['snomed']):\n",
    "        e_count+=1\n",
    "        for t in truth_entities:\n",
    "            if e.label_ == t['type'] and e.start_char == t['offsets'][0][0]-sentence_delta and e.end_char == t['offsets'][0][1]-sentence_delta:\n",
    "                tp+=1\n",
    "                break\n",
    "    t_count += len(truth_entities)\n",
    "\n",
    "\n",
    "json_path = \"/Users/leon.sarodnik/Documents/GitHub/ggponc_annotation/grassco_project_2023-01-05_0021/grassco_converted/json/fine/long/test.json\"\n",
    "\n",
    "with open(json_path) as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "for document in data:\n",
    "    print(\"Current document_id: \"+document['document_id'])\n",
    "    for sentence in document['passages']:\n",
    "        nlp_findings = nlp(sentence['text'], disable=[\"tok2vec\", \"tagger\", \"attribute_ruler\",\"lemmatizer\"]) # use pipe, disable_components in pipe (standard NER tagger etc.) - \"parser\" is helpful (10 matches less when not used)\n",
    "        manual_findings = find_annotated_entities(sentence, document)\n",
    "        compare_findings(nlp_findings, manual_findings, sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473c097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = t_count-tp\n",
    "fp = e_count-tp\n",
    "\n",
    "print(f'Actual Entities: {t_count}')\n",
    "print(f'Predicted Entities: {e_count}')\n",
    "\n",
    "print(f'True Positives: {tp}')\n",
    "print(f'False Positives: {fp}')\n",
    "print(f'False Negatives: {fn}')\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1_score:.2f}')\n",
    "\n",
    "#result for fine/long\n",
    "\n",
    "#Actual Entities: 6194\n",
    "#Predicted Entities: 5171\n",
    "#True Positives: 2584\n",
    "#False Positives: 2587\n",
    "#False Negatives: 3610\n",
    "#Precision: 0.50\n",
    "#Recall: 0.42\n",
    "#F1 Score: 0.45\n",
    "\n",
    "#result for fine/short\n",
    "#Actual Entities: 7201\n",
    "#Predicted Entities: 5171\n",
    "#True Positives: 1737\n",
    "#False Positives: 3434\n",
    "#False Negatives: 5464\n",
    "#Precision: 0.34\n",
    "#Recall: 0.24\n",
    "#F1 Score: 0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2f582c-bcf0-466c-99bb-6a06328cbb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sorted(list(doc.spans['snomed']), key=lambda s: s.start):\n",
    "    print(s, s.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e149811-36c4-4563-9480-0c47a296f9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
